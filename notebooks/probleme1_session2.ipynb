{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Problème - Session n°2"
      ],
      "metadata": {
        "id": "GYE9s4-HA1o_"
      },
      "id": "GYE9s4-HA1o_"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9B1KazoGQYLE"
      },
      "id": "9B1KazoGQYLE"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N5FpMt0_l-ej"
      },
      "id": "N5FpMt0_l-ej"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate Matrices M[k,n] (5x3) of size (10x10)\n",
        "k = 5\n",
        "n = 3\n",
        "dim = 10\n",
        "M = torch.randn(k, n, dim, dim)  # Random matrices"
      ],
      "metadata": {
        "id": "fxbj00y9l_cy"
      },
      "id": "fxbj00y9l_cy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Generate dataset\n",
        "N = 50000  # Total samples\n",
        "batch_size = 1000\n",
        "num_batches = N // batch_size\n",
        "\n",
        "torch.manual_seed(42)\n",
        "x_data = torch.randn(N, dim)\n",
        "L_data = torch.arange(num_batches).repeat_interleave(batch_size) % k  # L values shared within 1000s\n",
        "\n",
        "y_data = []\n",
        "for i in range(N):\n",
        "    L_i = L_data[i]\n",
        "    x_i = x_data[i]\n",
        "    y_i = torch.max(M[L_i,2] @ torch.relu(M[L_i,1] @ torch.relu(M[L_i,0] @ x_i)))\n",
        "    y_data.append(y_i)\n",
        "\n",
        "y_data = torch.tensor(y_data).unsqueeze(1)"
      ],
      "metadata": {
        "id": "FniSJgYmmJtx"
      },
      "id": "FniSJgYmmJtx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define Model\n",
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_batches):\n",
        "        super(DeepMLP, self).__init__()\n",
        "        self.theta = nn.Parameter(torch.randn(num_batches, 2))  # Latent variable\n",
        "        self.fc1 = nn.Linear(input_dim + 2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # self.fc6 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # self.fc7 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # self.fc8 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc7 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, indices):\n",
        "        theta_batch = self.theta[indices // 1000, :]\n",
        "        noise = torch.normal(mean=torch.zeros_like(theta_batch),\n",
        "                             std=torch.ones_like(theta_batch))\n",
        "        x = torch.cat([x, theta_batch + noise], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        # x = torch.relu(self.fc4(x))\n",
        "        # x = torch.relu(self.fc5(x))\n",
        "        # x = torch.relu(self.fc6(x))\n",
        "        # x = torch.relu(self.fc7(x))\n",
        "        # x = torch.relu(self.fc8(x))\n",
        "        x = self.fc7(x)\n",
        "        return x, theta_batch\n",
        "\n",
        "# Model and optimizer\n",
        "model = DeepMLP(input_dim=dim, hidden_dim=256, output_dim=1, num_batches=num_batches)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "iqOU5Z5qmZ7J"
      },
      "id": "iqOU5Z5qmZ7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "num_epochs = 2000\n",
        "for epoch in range(num_epochs):\n",
        "    perm = torch.randperm(N)\n",
        "    x_shuffled, y_shuffled, L_shuffled = x_data[perm], y_data[perm], L_data[perm]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, N, batch_size):\n",
        "        indices = perm[i:i+batch_size]\n",
        "        x_batch = x_shuffled[i:i+batch_size].cuda()\n",
        "        y_batch = y_shuffled[i:i+batch_size].cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred, theta_batch = model(x_batch, indices)\n",
        "        loss = criterion(y_pred, y_batch) + (torch.relu(theta_batch - 10) + torch.relu(-10 - theta_batch)).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KZybbELTmdOS"
      },
      "id": "KZybbELTmdOS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avec 5 couches:\n",
        "# Extract learned theta\n",
        "theta_learned = model.theta.detach().cpu().numpy()\n",
        "L_colors = np.array([i % k for i in range(num_batches)])\n",
        "\n",
        "# Plot latent space\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(theta_learned[:,0], theta_learned[:,1], c=L_colors, cmap='viridis',\n",
        "            alpha=0.7, s=2)\n",
        "plt.colorbar(label='L value')\n",
        "plt.xlabel('Theta 1')\n",
        "plt.ylabel('Theta 2')\n",
        "plt.title('Latent Space Visualization')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QnAXfO-qmyVx"
      },
      "id": "QnAXfO-qmyVx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partie III\n",
        "\n",
        "La solution proposée dans la partie II a un défaut: les performances chutent sur les paires d'antennes qui n'ont pas été rencontrées pendant l'entraînement. Pour le corriger, on se propose de suivre la méthode suivante:\n",
        "- ajouter un perceptron \"générique\" de même architecture que les perceptrons spécifiques.\n",
        "- entraîner le réseau de la partie II en remplaçant une fois sur quatre l'identifiant de la paire d'antennes par l'indice du perceptron générique.\n",
        "- après cinquante époques, geler les poids de la partie générique et prolonger l'apprentissage des perceptrons spécifiques sur une vingtaine d'époque.\n",
        "\n",
        "**Consignes:** \\\n",
        "\n",
        "1) Mettre en oeuvre cette méthode\n",
        "\n",
        "2) Conclure sur son efficacité"
      ],
      "metadata": {
        "id": "gLloZqn-umi8"
      },
      "id": "gLloZqn-umi8"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cTCs6AZ-e6dD"
      },
      "id": "cTCs6AZ-e6dD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}